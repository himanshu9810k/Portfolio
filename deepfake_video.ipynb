{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu9810k/Portfolio/blob/main/deepfake_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi5Mc-iof6Mw",
        "outputId": "33ba2ff3-e98d-4e91-c0f0-5b1c7f0efc9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GYOJftGxgF3S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/processed/fake', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/processed/real', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "31n2Bac1gUyI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "V2QdC7h4BKvv",
        "outputId": "8c816639-74b0-40b6-c642-f06c28ed7ace"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6383b5913b50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Train-test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data loaded and split successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import UnidentifiedImageError\n",
        "\n",
        "# Paths to preprocessed data\n",
        "data_dir = '/content/drive/MyDrive/processed'\n",
        "\n",
        "# Load real and fake data\n",
        "real_data = [f for f in os.listdir(data_dir + '/real') if f.endswith('.jpg')]\n",
        "fake_data = [f for f in os.listdir(data_dir + '/fake') if f.endswith('.jpg')]\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# Load and normalize real images\n",
        "for img in real_data:\n",
        "    img_array = img_to_array(load_img(data_dir + '/real/' + img, target_size=(128, 128))) / 255.0\n",
        "    X.append(img_array)\n",
        "    Y.append(1)\n",
        "\n",
        "# Load and normalize fake images\n",
        "for img in fake_data:\n",
        "    try:\n",
        "        img_array = img_to_array(load_img(data_dir + '/fake/' + img, target_size=(128, 128))) / 255.0\n",
        "        X.append(img_array)\n",
        "        Y.append(0)\n",
        "    except UnidentifiedImageError:\n",
        "        print(f\"Skipping invalid image: {data_dir}/fake/{img}\")\n",
        "        # Optionally, you can remove the invalid image from the fake_data list\n",
        "        # fake_data.remove(img)\n",
        "        # or move it to a separate folder for investigation\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# One-hot encode the labels\n",
        "Y = to_categorical(Y, 2)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=5)\n",
        "\n",
        "print(\"Data loaded and split successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csmCkYGLEDuB"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Conv2D, BatchNormalization, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define parameters\n",
        "activation = 'relu'\n",
        "padding = 'same'\n",
        "epsilon = 0.001\n",
        "# Define parameters\n",
        "l2_reg = 0.0001  # Reduced L2 regularization strength\n",
        "learning_rate = 0.0003  # Slightly increased learning rate\n",
        "droprate = 0.4  # Lower dropout rate\n",
        "\n",
        "# Define the input shape for your 128x128x3 images\n",
        "input_shape = (128, 128, 3)\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "# Convolutional layers with L2 regularization\n",
        "x = Conv2D(filters=8, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(inputs)\n",
        "x = BatchNormalization(epsilon=epsilon)(x)\n",
        "x = Conv2D(filters=16, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Conv2D(filters=16, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = BatchNormalization(epsilon=epsilon)(x)\n",
        "x = AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "x = Conv2D(filters=32, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Conv2D(filters=32, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Conv2D(filters=32, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = BatchNormalization(epsilon=epsilon)(x)\n",
        "x = AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "x = Conv2D(filters=64, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Conv2D(filters=64, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Conv2D(filters=64, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = Conv2D(filters=64, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = BatchNormalization(epsilon=epsilon)(x)\n",
        "x = AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "x = Conv2D(filters=128, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = BatchNormalization(epsilon=epsilon)(x)\n",
        "x = MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "x = Conv2D(filters=256, kernel_size=3, activation=activation, padding=padding, kernel_regularizer=l2(l2_reg))(x)\n",
        "x = BatchNormalization(epsilon=epsilon)(x)\n",
        "x = MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "# Global Average Pooling\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Fully connected layers with Dropout and L2 regularization\n",
        "x = Dropout(droprate)(x)\n",
        "x = Dense(32, kernel_regularizer=l2(l2_reg), activation='relu')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Dropout(droprate)(x)\n",
        "x = Dense(16, kernel_regularizer=l2(l2_reg), activation='relu')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Dropout(droprate)(x)\n",
        "\n",
        "# Output layer (2 units for binary classification)\n",
        "outputs = Dense(2, kernel_regularizer=l2(l2_reg), activation='sigmoid')(x)\n",
        "\n",
        "# Create model\n",
        "model2 = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ys_1DHyIvN-"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model with adjusted learning rate\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to monitor accuracy instead of loss\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Define the number of epochs\n",
        "EPOCHS = 30\n",
        "\n",
        "# Train the model with data augmentation and new early stopping settings\n",
        "history = model2.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
        "                     epochs=EPOCHS,\n",
        "                     validation_data=(X_val, Y_val),\n",
        "                     callbacks=[early_stopping],\n",
        "                     verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjDPwZ2GJ0Ky"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the actual number of epochs the model trained for\n",
        "EPOCHS_TRAINED = len(history.history['accuracy'])\n",
        "\n",
        "# Create subplots for accuracy and loss\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))\n",
        "t = f.suptitle('Custom CNN Architecture Performance', fontsize=12)\n",
        "f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "epoch_list = list(range(1, EPOCHS_TRAINED+1))  # Update epoch_list to match the actual trained epochs\n",
        "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy', marker='o')\n",
        "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "ax1.set_xticks(np.arange(0, EPOCHS_TRAINED+1, 1))  # Update x-axis ticks\n",
        "ax1.set_ylabel('Accuracy Value')\n",
        "ax1.set_xlabel('Epoch #')\n",
        "ax1.set_title('Accuracy')\n",
        "l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(epoch_list, history.history['loss'], label='Train Loss', marker='o')\n",
        "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "ax2.set_xticks(np.arange(0, EPOCHS_TRAINED+1, 1))  # Update x-axis ticks\n",
        "ax2.set_ylabel('Loss Value')\n",
        "ax2.set_xlabel('Epoch #')\n",
        "ax2.set_title('Loss')\n",
        "l2 = ax2.legend(loc=\"best\")\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to preprocess a single frame\n",
        "def preprocess_frame(frame, target_size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Preprocess the frame: resize and normalize.\n",
        "    \"\"\"\n",
        "    frame_resized = cv2.resize(frame, target_size)  # Resize to the input size of the model\n",
        "    frame_normalized = frame_resized / 255.0  # Normalize the pixel values\n",
        "    return np.expand_dims(frame_normalized, axis=0)  # Add batch dimension\n",
        "\n",
        "# Function to predict on a single video\n",
        "def predict_video(video_path, model, target_size=(128, 128), num_frames_to_sample=30):\n",
        "    \"\"\"\n",
        "    Predict whether the video is real or fake by sampling frames and averaging predictions.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames = []\n",
        "\n",
        "    # Sample frames uniformly across the video\n",
        "    frame_indices = np.linspace(0, frame_count - 1, num=num_frames_to_sample, dtype=int)\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # Set the video to the current frame index\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue  # If frame can't be read, skip it\n",
        "        processed_frame = preprocess_frame(frame, target_size)\n",
        "        frames.append(processed_frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Convert list of frames to a numpy array\n",
        "    frames = np.vstack(frames)\n",
        "\n",
        "    # Predict for each frame\n",
        "    predictions = model.predict(frames)\n",
        "\n",
        "    # Average the predictions to get the final prediction\n",
        "    avg_prediction = np.mean(predictions, axis=0)\n",
        "\n",
        "    # Return the final prediction: 1st output corresponds to 'real', 2nd to 'fake'\n",
        "    return \"Fake\" if avg_prediction[1] > avg_prediction[0] else \"Real\", avg_prediction\n",
        "\n",
        "# Path to the video file\n",
        "video_path = '/content/v31.mp4'  # Replace with the path to the video you want to predict\n",
        "\n",
        "# Use the current model (model2) for prediction\n",
        "prediction, avg_prediction = predict_video(video_path, model2)\n",
        "print(f\"The video is predicted as: {prediction}\")\n",
        "print(f\"Prediction scores: {avg_prediction}\")\n"
      ],
      "metadata": {
        "id": "uCyfcUf0fhQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_model_performance(history):\n",
        "    # Convert history to a dataframe for easier plotting\n",
        "    import pandas as pd\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "\n",
        "    # Set the style and color palette\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Create a figure for plotting\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Plot training & validation accuracy\n",
        "    sns.lineplot(data=history_df[['accuracy', 'val_accuracy']], ax=ax1)\n",
        "    ax1.set_title('Model Accuracy', fontsize=16)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax1.legend(['Train', 'Validation'], loc='best')\n",
        "\n",
        "    # Plot training & validation loss\n",
        "    sns.lineplot(data=history_df[['loss', 'val_loss']], ax=ax2)\n",
        "    ax2.set_title('Model Loss', fontsize=16)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Loss', fontsize=12)\n",
        "    ax2.legend(['Train', 'Validation'], loc='best')\n",
        "\n",
        "    # Adjust layout and display the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming `history` is the object returned by model.fit()\n",
        "visualize_model_performance(history)\n"
      ],
      "metadata": {
        "id": "SA0ezlyMiVDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model2.save('deepfake_detection_model.keras')\n"
      ],
      "metadata": {
        "id": "MRSdnkp6iqfg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}